\documentclass[a4paper,11pt]{article}

%% ===== Instructions reminder =====

%% - The main purpose of this project is to evaluate your approach to
%% formalization, and your comments on the value of the solutions you
%% tried. Investigating several possible definitions/proof methods is a
%% good thing and we value yoru description of the problems encountered
%% and of the solutions you eventually preferred as much as the the
%% number of questions you answer too. The best mark will not necessarily
%% go to the project answering the maximum number of questions.

%% - Do not hesitate to insert comments in your code...

%% - The report should be a separate, readable document, ideally in pdf
%% format, written in English or in French. It should not exceed 5 pages
%% of a reasonably readable document style (no magnifying glass
%% needed). It can refer to the code but should explain the approach, the
%% problems encountered, and comment the code submitted: is it
%% satisfactory (data-structures, statements, proof scripts) or
%% were some proofs more difficult (or easier!) formally than on paper,
%% what was difficult and why, what ingredient or what method worked
%% well, did you use/lack of some automation tools...

%% ===== Basis for the report =====

%% - First, an important note: the file structure of the code.
%% - The choice of indexing variables: types and kinds are numbered
%%   separately, etcetera. Why we chose it.
%% - We started defining typing and kinding as Fixpoints instead of
%%   Inductive predicates. This made everything really hard!
%% - Equality for types: it's not enough to have =, we needed beq_typ
%%   because you can't use = in if...then...else... (and this is necessary for
%%   type_of, for example). This means we also needed some lemmas (like the ones
%%   in the exam!) showing correctness (but not completeness!) for this
%%   equality, beq_typ
%% - We got the wrong definition for insert_kind. This was extremely
%%   painful, because we got it wrong in subtle ways so many theorems were still
%%   true, but not all, and it took us a while to realize that it was wrong. You
%%   should probably check the Git logs to see the previous version. The proofs
%%   were HUGE and very different. This is very interesting: things like "does
%%   this type have a kind?" were based in "*how many* free variables does it
%%   have?" and "*how many* kinds are in the environment?" This is
%%   interesting. The new way of doing this is through some "extensionality"
%%   lemmas that say that "the kind of a type only depends on the kinds in the
%%   environment". Take a look at these and understand how powerful they are!
%%   Same for typing, but typing extensionality is more complex! Again, read the
%%   code, understand the lemmas. Maybe it's interesting if you go backwards:
%%   look at the final theorems (the ones that are required by the exercise) and
%%   see how I defined lemmas in order to prove the required stuff!
%% - Reduction.v is very interesting because I started doing advanced
%%   stuff: you have examples of notations and tactics. It's worth mentioning
%%   the tactics (they are easy to understand) and how they make the congruence
%%   lemmas very easy to prove.
%% - Also, please, if you could format the files nicely and group related
%%   stuff together and put comments, that would be nice. If not, just add a
%%   header to each file saying "this file contains this and that, for this
%%   exercise".
%% - StrongNormalization.v is incomplete...and is never going to be
%%   completed. The incomplete code is in the comments.
%% - Some cool tactics and stuff:
%% > - *eauto, eapply, erewrite* - they usually avoid uses of 'specialize'
%% > - *pose* - good when you want to take a lemma from the outside and put
%% >   it in your proof
%% > - *auto/eauto with arith, auto/eauto using [lemma_1, lemma_2...]* -
%% >   good for when the proof is trivial but you need to remember a few lemmas
%% >   that are defined outside. It avoids you having to pose the lemmas.
%% > - The tactic language: *destruct; someTactic.* will apply someTactic
%% >   to all branches of the destruct/case/induction in parallel! This saves a
%% >   lot of inductions followed by intros in every branch or stuff like that.
%% > - *try* allows you to apply a tactic and, if it doesn't work, it's ok.
%% >   this is good because some tactics like omega or discriminate will fail if
%% >   they don't work! you can't just tell coq "use omega everywhere and only ask
%% >   me to prove the cases where it doesn't work", because if it doesn't work it
%% >   will raise an error. Instead, "use *try omega *everywhere". Instead of
%% >   failing, it will let you continue the proof.
%% > - *discriminate/try discriminate* - it's good when you have a case
%% >   analysis but some cases are impossible - like if you know some term CAN'T
%% >   be a variable. Discriminate will take care of those cases.
%% >
%% >

% We may want to use this if we get more than 5 pages
%\usepackage[a4paper,includeheadfoot,margin=2.4cm,footskip=.5cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}

\begin{document}
\title{Implementing Stratified System F in Coq}
\author{M. Alvarez and Y. Juglaret}
\date{}
\maketitle

For this project, we formalized a predicative variant of System F and
proved interesting properties about it, mostly based on the paper
\emph{Hereditary Substitution for Stratified System F} by H. D. Eades
III and A. Stump.

In this document, we will first describe the architecture of the code
used for the project. Then, we will give more details about our
approach for each part. We think that following the sequential order
of the project description is interesting, because it is the order in
which we wrote the code. As such, it is also the order for our growth
in experience. We will finally conclude by taking a larger look at our
solution as a whole and criticizing it.

\section*{Overview}

As the script file grew larger and larger, we decided to split it into
several distinct files that group altogether related code. We thus
have:

\begin{itemize}
  \item \verb|SysF.v| and \verb|Correctness.v| for part \emph{I.1.2
    Definitions} ;
  \item \verb|Metatheory.v| for part \emph{I.1.3 Basic Metatheory} ;
  \item \verb|Reduction.v| for part \emph{I.2 Reduction and Normal
    Terms} ;
  \item \verb|StrongNormalization.v| for parts \emph{II.1 Strong
    Normalization} and \emph{II.2 Hereditary Substitution}.
\end{itemize}

We provide compilation rules for each distinct file, which helps
for sanity check. That is, running \verb|make| will compile all files
respecting the order of dependencies, and thus fail if anything is
wrong within the whole project.

Because of these dependencies, we strongly recommend running
\verb|make| once before trying to run one of the files. This way, all
dependencies will be directly available.

\section{Definitions}

In file \verb|SysF.v|, we define our language by providing Coq
datatypes for reasoning about terms, types, kinds and environments. We
then provide helper functions. Finally, we provide predicates for
typing and kinding as well as a kinding and a typing algorithm.

In file \verb|Correctness.v|, we prove correctness (and sometimes
completeness) properties about some of these functions and algorithms.

Let us now comment the design choices that we made for this part and
the problems that we faced.

\subsection*{Kinds, Types and Environments}

We chose to represent kinds and types using de Bruijn indices, with
one counter for kinds and one for types. Environments are then
represented as a single context, using an inductive type \verb|env|
which is isomorphic to \verb|list (kind + typ)|.

This way, the environment itself carries along the order of
definitions, which is required for example when checking that an
environment is well-formed. This is not the case when environments are
represented as distinct contexts by using a type isomorphic to
\verb|list kind * list typ|.

\subsection*{Structural and Decidable Equality over Types}

We had to define a structural and decidable egality predicate over
types \verb|beq_typ|. We need it for example in \verb|type_of|, which
tries to assign a type to a term. In this function, we need to test
for equality in a conditional statement. In Coq, the usual equality
predicate is \verb|eq|, which is a predicate with results in
\verb|Prop|. Such a predicate only gives back a logical proposition
for which we have no clue whether it is true or false.

Of course, like in the exam with the structural decidable equality over
polynomials, this calls for the addition of some lemmas talking about
\verb|beq_typ| if we want to prove any interesting property about a
function that uses it. In particular, in \verb|Correctness.v| we
define two lemmas that state that \verb|beq_typ| is reflexive and,
more interestingly, correct.

\subsection*{Kinding and Typing Predicates}

On paper, the typing and kinding predicates are defined by deductive
rules. In our early implementations, we described them as recursive
predicates using \verb|Fixpoint| declarations. This was a bad idea,
and proving anything related to these predicates required a lot
of efforts.

After a while, we learned about inductive predicates and how
appropriate they are for representing deductive rules in Coq. We thus
switched to \verb|Inductive| definitions for both, and thanks to the
inductive reasoning all properties became a lot easier to prove. Even
though this change implied having to adapt all of the proofs that we
had already done, in the end we have probably still gained time thanks
to it.

Notice that we still have some logical predicates defined using
\verb|Fixpoint|. It is for example the case of \verb|wf_typ| (and
\verb|wf_env|), which states the well-formedness of types
(resp. environments). This is OK, because this definition is made
by case on the type (resp. the environment). Thus, when we have a
proof \verb|p : wf_typ e T| (resp. \verb|p : wf_env e|),
we can reason by case on \verb|T| (resp. \verb|e|) to get the
shape of \verb|p|. If we defined these predicates using
\verb|Inductive|, we would instead reason by case on \verb|p| to get
the shape of \verb|T| (resp. \verb|e|). So in these cases, the two
possible definitions are somehow equivalent.

\section{Basic Metatheory}

In file \verb|Metatheory.v|, we prove regularity and narrowing for our
system. This requires a lot of lemmas, and some proofs are very
long. In fact, parts of these could probably be automatized using
techniques that we will describe in the next part. Yet, we think that
it is good to keep these long proofs in the project since it allows
for comparison with the automatized ones.

Let us now describe the biggest problem that we faced in this part.

\subsection*{Inserting a Kind in an Environment}

The predicate \verb|insert_kind| states that some environment is the
result of inserting a kind in another environment at a given position.
Our first definition for this predicate was wrong, but in subtle
ways. Thus, many of the theorems were still true (but not all of
them), and it took us a while to realize that in fact the definition
itself was wrong.

Moreover, the proofs back then were very different, because they
relied on different properties, and really huge. For example,
knowing if a type had a given kind required knowing how many free
variables this type had. Now, we can apply extensionality which
states that the kind of a type only depends on the kinds in the
environment. The same ideas apply to typing, with a more complex
notion of extensionality.

\section{Reduction and Normal Terms}

In file \verb|Reduction.v|, we study reduction. We fist define our
reduction relation together with its (reflexive and usual) transitive
closures. Then, we give inductive predicates that characterize normal
and neutral terms and prove that this characterization is correct and
complete. Finally, we prove that type substitution preserves normality
and neutralily.

When we reached that point of the project, we thought that learning
more about advanced Coq techniques could at the same time be
interesting and allow us to gain some time. Thus, in addition to
notations, \verb|Reduction.v| uses tactics that we defined ourselves,
allowing for some amount of automation.

This automation was very useful and allowed us to write concise
proofs. It allows to take away the trivial parts of the proofs so that
the writer (and later the reader) can focus on the interesting parts.

Let us now describe these tactics.

\subsection{Proving a Reduction}

The first tactic that we implemented, \verb|one_step|, allows to prove
a one-step reduction when we have sufficient knowledge about the terms
involved to conclude. In order to do this, it tries to \verb|eapply|
each of the constructor of the inductive predicate that defines our
reduction. In the case of a reduction in a context, it performs a
recursive tactic call to prove it. Despite recursion, this tactic
always terminates because the terms involved in the recursion are
always strictly smaller than the former arguments.

The second tactic that we implemented, \verb|steps|, allows to prove
a reduction in zero, one or several steps. It works like \verb|step|,
but for this tactic the recursive call may not terminate: it will
go on until we succeed in proving that there is indeed a n-step
reduction between the terms involved, and thus loop if we cannot
succeed in proving it. Because of this, we first implemented another
tactic, \verb|steps_unbound|, which may not terminate. Then, we
defined \verb|steps| as applying the \verb|steps_unbound| tactic with
a time limit.

In fact, it could be more interesting to have the user provide a limit
on the number of recursive calls, rather than force an arbitrary limit
on the time of the execution. This would allow the tactic to have a
deterministic behavior. However, we think that our solution is
satisfactory enough for the purpose of the project.

\subsection{Proving Normality and Neutrality}

We then implemented two tactics, \verb|neutralize| and
\verb|normalize|, which allow to prove that a term is neutral
(resp. normal) when we have sufficient knowledge about it to conclude.
Like step, they work by trying to apply the constructors of the
inductive predicate, with some potential recursive calls that always
terminate.

\section{Strong Normalization}

In \verb|StrongNormalization.v|, we provide some answers that could
lead to the proof of strong normalization for our system. This part is
not finished, and some proofs contain admitted fragments. However,
some interesting lemmas are already proved.

\section*{Conclusion}

[...]

\end{document}
