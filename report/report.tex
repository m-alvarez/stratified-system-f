\documentclass[a4paper,11pt]{article}

%% ===== Instructions reminder =====

%% - The main purpose of this project is to evaluate your approach to
%% formalization, and your comments on the value of the solutions you
%% tried. Investigating several possible definitions/proof methods is a
%% good thing and we value your description of the problems encountered
%% and of the solutions you eventually preferred as much as the the
%% number of questions you answer too. The best mark will not necessarily
%% go to the project answering the maximum number of questions.

%% - Do not hesitate to insert comments in your code...

%% - The report should be a separate, readable document, ideally in pdf
%% format, written in English or in French. It should not exceed 5 pages
%% of a reasonably readable document style (no magnifying glass
%% needed). It can refer to the code but should explain the approach, the
%% problems encountered, and comment the code submitted: is it
%% satisfactory (data-structures, statements, proof scripts) or
%% were some proofs more difficult (or easier!) formally than on paper,
%% what was difficult and why, what ingredient or what method worked
%% well, did you use/lack of some automation tools...

%% ===== Basis for the report =====

%% - First, an important note: the file structure of the code.
%% - The choice of indexing variables: types and kinds are numbered
%%   separately, etcetera. Why we chose it.
%% - We started defining typing and kinding as Fixpoints instead of
%%   Inductive predicates. This made everything really hard!
%% - Equality for types: it's not enough to have =, we needed beq_typ
%%   because you can't use = in if...then...else... (and this is necessary for
%%   type_of, for example). This means we also needed some lemmas (like the ones
%%   in the exam!) showing correctness (but not completeness!) for this
%%   equality, beq_typ
%% - We got the wrong definition for insert_kind. This was extremely
%%   painful, because we got it wrong in subtle ways so many theorems were still
%%   true, but not all, and it took us a while to realize that it was wrong. You
%%   should probably check the Git logs to see the previous version. The proofs
%%   were HUGE and very different. This is very interesting: things like "does
%%   this type have a kind?" were based in "*how many* free variables does it
%%   have?" and "*how many* kinds are in the environment?" This is
%%   interesting. The new way of doing this is through some "extensionality"
%%   lemmas that say that "the kind of a type only depends on the kinds in the
%%   environment". Take a look at these and understand how powerful they are!
%%   Same for typing, but typing extensionality is more complex! Again, read the
%%   code, understand the lemmas. Maybe it's interesting if you go backwards:
%%   look at the final theorems (the ones that are required by the exercise) and
%%   see how I defined lemmas in order to prove the required stuff!
%% - Reduction.v is very interesting because I started doing advanced
%%   stuff: you have examples of notations and tactics. It's worth mentioning
%%   the tactics (they are easy to understand) and how they make the congruence
%%   lemmas very easy to prove.
%% - Also, please, if you could format the files nicely and group related
%%   stuff together and put comments, that would be nice. If not, just add a
%%   header to each file saying "this file contains this and that, for this
%%   exercise".
%% - StrongNormalization.v is incomplete...and is never going to be
%%   completed. The incomplete code is in the comments.
%% - Some cool tactics and stuff:
%% > - *eauto, eapply, erewrite* - they usually avoid uses of 'specialize'
%% > - *pose* - good when you want to take a lemma from the outside and put
%% >   it in your proof
%% > - *auto/eauto with arith, auto/eauto using [lemma_1, lemma_2...]* -
%% >   good for when the proof is trivial but you need to remember a few lemmas
%% >   that are defined outside. It avoids you having to pose the lemmas.
%% > - The tactic language: *destruct; someTactic.* will apply someTactic
%% >   to all branches of the destruct/case/induction in parallel! This saves a
%% >   lot of inductions followed by intros in every branch or stuff like that.
%% > - *try* allows you to apply a tactic and, if it doesn't work, it's ok.
%% >   this is good because some tactics like omega or discriminate will fail if
%% >   they don't work! you can't just tell coq "use omega everywhere and only ask
%% >   me to prove the cases where it doesn't work", because if it doesn't work it
%% >   will raise an error. Instead, "use *try omega *everywhere". Instead of
%% >   failing, it will let you continue the proof.
%% > - *discriminate/try discriminate* - it's good when you have a case
%% >   analysis but some cases are impossible - like if you know some term CAN'T
%% >   be a variable. Discriminate will take care of those cases.
%% >
%% >

% We may want to use this if we get more than 5 pages
%\usepackage[a4paper,includeheadfoot,margin=2.4cm,footskip=.5cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
%\usepackage{amsmath}
%\usepackage{amsthm}
%\usepackage{amsfonts}

\begin{document}
\title{Implementing Stratified System F in Coq}
\author{M. Alvarez and Y. Juglaret}
\date{}
\maketitle

For this project, we formalized a predicative variant of System F and
proved interesting properties about it, mostly based on the paper
\emph{Hereditary Substitution for Stratified System F} by H. D. Eades
III and A. Stump.

In this document, we will first describe the architecture of the code
used for the project. Then, we will give more details about our
approach for each part. We think that following the sequential order
of the project description is interesting, because it is the order in
which we wrote the code. As such, it is also the order for our growth
in experience.

\section{Architecture of the Project}

As the script file grew larger and larger, we decided to split it into
several distinct files that group altogether related code. We thus
provide :

\begin{itemize}
  \item \verb|SysF.v| and \verb|Correctness.v| for part \emph{I.1.2
    Definitions} ;
  \item \verb|Metatheory.v| for part \emph{I.1.3 Basic Metatheory} ;
  \item \verb|Reduction.v| for part \emph{I.2 Reduction and Normal
    Terms} ;
  \item \verb|StrongNormalization.v| for the optional
    part \emph{II.1 Strong Normalization}.
\end{itemize}

We provide compilation rules for each distinct file, which helps
for sanity check. That is, running \verb|make| will compile all files
respecting the order of dependencies, and thus fail if anything is
wrong within the whole project.

Because of these dependencies, we strongly recommend running
\verb|make| once before trying to run one of the files. This way, all
dependencies will directly be available.

\section{Definitions}

\subsection{Kinds, Types and Environments}

We chose represent kinds and types using de-Bruijn indices, with
one counter for kinds and one for types. Environments are then
represented as a single context, using an inductive type which is
isomorphic to \verb|list (kind + typ)|.

This allows to know directly the order of definitions, which is
required for example when checking that an environment is
well-formed. This is not the case when environments are represented as
distinct contexts by using a type isomorphic to
\verb|list kind * list typ|.

\subsection{Structural and Decidable Equality over Types}

We defined a structural and decidable egality predicate over types:

\begin{verbatim}
beq_typ : typ -> typ -> bool
\end{verbatim}

We need it for example in the function:

\begin{verbatim}
type_of : env -> term -> option typ
\end{verbatim}

In this function, we need to test for equality in a conditional
statement. In coq, the usual equality predicate is \verb|eq|, which is
a predicate with results in \verb|Prop|. That is, such a predicate
only gives back a logical proposition for which we have no clue
whether it is true or false.

Of course, like in the exam with the structural decidable equality over
polynomials, this calls for the addition of some lemmas talking about
\verb|beq_typ| if we want to prove any interesting property about a
function that uses it. In particular, in \verb|Correctness.v| we
define two lemmas that state that \verb|beq_typ| is correct and
reflexive.

\subsection{Kinding and Typing Predicates}

On paper, the typing and kinding predicates are defined by deductive
rules. In our early implementations, we described them as recursive
predicates using \verb|Fixpoint| declarations. This was a very bad
idea, and proving anything related to these predicates required a lot
of efforts.

After a while, we learned about inductive predicates and how
appropriate they are for representing deductive rules in Coq. We thus
switched to \verb|Inductive| definitions for both, and thanks to the
inductive reasoning all properties became a lot easier to prove. Even
though this change implied having to adapt all of the proofs that we
had already done, in the end we have probably still gained time thanks
to it.

\section{Basic Metatheory}

[...]

\section{Reduction and Normal Terms}

When we reached that point of the project, we thought that learning
more about advanced Coq techniques could at the same time be
interesting and allow us to gain some time. Thus, \verb|Reduction.v|
uses notations and tactics that we defined ourselves.

[...]

\section{Strong Normalization}

[...]

\end{document}
